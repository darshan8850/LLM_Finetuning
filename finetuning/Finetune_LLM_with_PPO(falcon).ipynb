{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m596fBppRsv"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.5.0\n",
        "!pip install -q sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import RewardTrainer"
      ],
      "metadata": {
        "id": "qwYGm8mYpZuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "def format(example):\n",
        "\n",
        "    # Format instruction\n",
        "    prompt = example['prompt']\n",
        "\n",
        "    # Format chosen answer\n",
        "    chosen = example['answer2']\n",
        "\n",
        "    # Format rejected answer\n",
        "    rejected = example['answer1']\n",
        "\n",
        "    return {\n",
        "        \"instruction\": prompt,\n",
        "        \"chosen_response\": chosen,\n",
        "        \"rejected_response\": rejected,\n",
        "    }\n",
        "\n",
        "# Load dataset\n",
        "\n",
        "generated_examples = pd.read_csv('/content/new_df')\n",
        "column_to_drop = 'Unnamed: 0'\n",
        "generated_examples.drop(column_to_drop, axis=1, inplace=True)\n",
        "generated_examples.dropna(axis=0, how='any', inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "dataset  = Dataset.from_pandas(generated_examples)\n",
        "\n",
        "# Save columns\n",
        "original_columns = dataset.column_names\n",
        "\n",
        "# Format dataset\n",
        "dataset = dataset.map(format,\n",
        "    remove_columns=original_columns\n",
        ")\n",
        "\n",
        "# Print sample\n",
        "dataset[1]"
      ],
      "metadata": {
        "id": "zdqBa229pZrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "4m3jz_6HpZpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "\n",
        "# Load tokenizer for the \"tiiuae/falcon-7b-instruct\" model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\")\n",
        "\n",
        "# Prepare quantization parameters\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=False, load_in_4bit=True)\n",
        "\n",
        "# Initialize the sequence classification model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"tiiuae/falcon-7b-instruct\",\n",
        "    quantization_config=quantization_config,  # Apply the quantization configuration\n",
        "    device_map={\"\": 0},  # Assign the model to device 0\n",
        "    trust_remote_code=True,  # Trust remote code\n",
        "    num_labels=1,  # Set the number of labels for classification (in this case, 1)\n",
        ")\n",
        "\n",
        "# Disable cache in model configuration\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "zZb_B_OBpZmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the tokenizer's pad_token is not set, use eos_token as pad_token and update model's pad_token_id\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "\n",
        "# Define a formatting function for processing examples\n",
        "def formatting_func(examples):\n",
        "    kwargs = {\n",
        "        \"padding\": \"max_length\",\n",
        "        \"truncation\": True,\n",
        "        \"max_length\": 512,\n",
        "        \"return_tensors\": \"pt\",\n",
        "    }\n",
        "\n",
        "    # Prepend the instruction and a line break to the chosen_response and rejected_response fields.\n",
        "    prompt_plus_chosen_response = (\n",
        "        examples[\"instruction\"] + \"\\n\" + examples[\"chosen_response\"]\n",
        "    )\n",
        "    prompt_plus_rejected_response = (\n",
        "        examples[\"instruction\"] + \"\\n\" + examples[\"rejected_response\"]\n",
        "    )\n",
        "\n",
        "    # Tokenize the modified fields.\n",
        "    tokens_chosen = tokenizer.encode_plus(prompt_plus_chosen_response, **kwargs)\n",
        "    tokens_rejected = tokenizer.encode_plus(prompt_plus_rejected_response, **kwargs)\n",
        "\n",
        "    return {\n",
        "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0],\n",
        "        \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
        "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0],\n",
        "        \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0],\n",
        "    }\n",
        "\n",
        "\n",
        "# Apply the formatting function to the prepared dataset\n",
        "formatted_dataset = dataset.map(formatting_func)\n",
        "\n",
        "# Split the formatted dataset into training and testing sets\n",
        "formatted_dataset = formatted_dataset.train_test_split()"
      ],
      "metadata": {
        "id": "jcFKJYCRpZj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from peft import LoraConfig\n",
        "from trl import RewardTrainer\n",
        "\n",
        "# Prepare training parameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./train_logs\",  # Output folder\n",
        "    max_steps=100,  # Maximum number of training steps\n",
        "    per_device_train_batch_size=4,  # Batch size per GPU for training\n",
        "    gradient_accumulation_steps=1,  # Number of steps to accumulate gradients\n",
        "    learning_rate=1.0e-4,  # Learning rate\n",
        "    optim=\"adamw_torch\",  # Optimizer\n",
        "    save_steps=50,  # How often to save checkpoints\n",
        "    logging_steps=10,  # How often to log training information\n",
        "    report_to=\"tensorboard\",  # Reporting method (in this case, TensorBoard)\n",
        "    remove_unused_columns=False,  # Whether to remove unused columns\n",
        "    evaluation_strategy=\"steps\",  # Evaluation strategy\n",
        "    num_train_epochs=5,  # Number of training epochs\n",
        ")\n",
        "\n",
        "# Prepare PEFT parameters\n",
        "peft_config = LoraConfig(\n",
        "    r=16,  # Value of r\n",
        "    lora_alpha=16,# Value of lora_alpha\n",
        "    target_modules=[\n",
        "\"query_key_value\",\n",
        "\"dense\",\n",
        "\"dense_h_to_4h\",\n",
        "\"dense_4h_to_h\",\n",
        "],\n",
        "    bias=\"none\",  # Bias setting\n",
        "    task_type=\"SEQ_CLS\",  # Task type (Sequence Classification)\n",
        "    modules_to_save=[\"scores\"],  # Modules to save\n",
        ")\n",
        "\n",
        "# Prepare RewardTrainer\n",
        "trainer = RewardTrainer(\n",
        "    model=model,  # The model for reinforcement learning\n",
        "    tokenizer=tokenizer,  # The tokenizer for processing input data\n",
        "    args=training_args,  # Training arguments\n",
        "    train_dataset=formatted_dataset[\"train\"],  # Training dataset\n",
        "    eval_dataset=formatted_dataset[\"test\"],  # Evaluation dataset\n",
        "    peft_config=peft_config,  # PEFT configuration\n",
        "    max_length=512,  # Maximum length of input\n",
        ")\n",
        "\n",
        "# Execute training\n",
        "trainer.train()\n",
        "\n",
        "# Save the pretrained reward model\n",
        "trainer.model.save_pretrained(\"./reward_model\")"
      ],
      "metadata": {
        "id": "La_e_b-_pZhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def get_score(model, tokenizer, prompt, response):\n",
        "    \"\"\"\n",
        "    Computes a score for a given prompt and response using a provided model and tokenizer.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model for scoring.\n",
        "        tokenizer: The tokenizer for processing input data.\n",
        "        prompt (str): The prompt text.\n",
        "        response (str): The response text.\n",
        "\n",
        "    Returns:\n",
        "        float: The computed score.\n",
        "    \"\"\"\n",
        "    print(prompt, response)\n",
        "    # Tokenize the input sequences\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        prompt,\n",
        "        response,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda:0\")\n",
        "\n",
        "    # Perform forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs,return_dict=True)\n",
        "\n",
        "    # Extract the logits\n",
        "    logits = outputs.logits\n",
        "\n",
        "    return logits.item()"
      ],
      "metadata": {
        "id": "QoodhaPGpZft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = 40\n",
        "dataset[x]"
      ],
      "metadata": {
        "id": "aHYdBlLOpZdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the prompt and responses for the example\n",
        "prompt = dataset[x][\"instruction\"]\n",
        "rejected_response = dataset[x][\"rejected_response\"]\n",
        "chosen_response = dataset[x][\"chosen_response\"]\n",
        "\n",
        "# Get the score for the example with the less preferred response\n",
        "score_less_pref = get_score(model, tokenizer, prompt, rejected_response)\n",
        "print(f\"Score for less preferred response: {score_less_pref}\")\n",
        "\n",
        "# Get the score for the example with the preferred response\n",
        "score_pref = get_score(model, tokenizer, prompt, chosen_response)\n",
        "print(f\"Score for preferred response: {score_pref}\")"
      ],
      "metadata": {
        "id": "YP1pSN7TpZbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "generated_examples = pd.read_csv('/content/new_df')\n",
        "column_to_drop = 'Unnamed: 0'\n",
        "generated_examples.drop(column_to_drop, axis=1, inplace=True)\n",
        "\n",
        "train_df, val_df = train_test_split(generated_examples, test_size=0.4, random_state=42)\n",
        "\n",
        "from datasets import Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "train_dataset=train_dataset.remove_columns('__index_level_0__')\n"
      ],
      "metadata": {
        "id": "Xe40T-he1LiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from trl.core import LengthSampler"
      ],
      "metadata": {
        "id": "YEHvfEcVMQSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = PPOConfig(\n",
        "    model_name=\"tiiuae/falcon-7b-instruct\",\n",
        "    learning_rate=1.41e-5,\n",
        ")\n",
        "\n",
        "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}"
      ],
      "metadata": {
        "id": "T_O73-tDMUgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "def build_dataset(config, df, input_min_text_length=2, input_max_text_length=200):\n",
        "    \"\"\"\n",
        "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
        "    customize this function to train the model on its own dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (`str`):\n",
        "            The name of the dataset to be loaded.\n",
        "\n",
        "    Returns:\n",
        "        dataloader (`torch.utils.data.DataLoader`):\n",
        "            The dataloader for the dataset.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "    generated_examples = df\n",
        "    column_to_drop = 'Unnamed: 0'\n",
        "    generated_examples.drop(column_to_drop, axis=1, inplace=True)\n",
        "\n",
        "    train_df, val_df = train_test_split(generated_examples, test_size=0.4, random_state=42)\n",
        "\n",
        "    ds = Dataset.from_pandas(train_df)\n",
        "\n",
        "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
        "\n",
        "    def tokenize(sample):\n",
        "        sample[\"input_ids\"] = tokenizer.encode(sample[\"prompt\"])[: input_size()]\n",
        "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "        return sample\n",
        "\n",
        "    ds = ds.map(tokenize, batched=False)\n",
        "    ds.set_format(type=\"torch\")\n",
        "    return ds"
      ],
      "metadata": {
        "id": "fFa0-AFFMaY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_examples = pd.read_csv('/content/new_df')\n",
        "dataset = build_dataset(config,df=generated_examples)"
      ],
      "metadata": {
        "id": "6LExcHqKMaIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.remove_columns('__index_level_0__')"
      ],
      "metadata": {
        "id": "1GMyPbsbMaFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
        "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "SDV_Eb9GMaCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n",
        "print(f'Collator input: {test_data}')\n",
        "print(f'Collator output: {collator(test_data)}')"
      ],
      "metadata": {
        "id": "AInhgnIQpZW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)"
      ],
      "metadata": {
        "id": "aCdVQxxEOuuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = ppo_trainer.accelerator.device\n",
        "if ppo_trainer.accelerator.num_processes == 1:\n",
        "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug"
      ],
      "metadata": {
        "id": "YpW-WS8QPCyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_kwargs = {\"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id}"
      ],
      "metadata": {
        "id": "vmzeFRzrPSe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "generated_examples = pd.read_csv('/content/new_df')\n",
        "column_to_drop = 'Unnamed: 0'\n",
        "generated_examples.drop(column_to_drop, axis=1, inplace=True)\n",
        "\n",
        "train_df, val_df = train_test_split(generated_examples, test_size=0.4, random_state=42)\n",
        "\n",
        "from datasets import Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "dataset=train_dataset.remove_columns('__index_level_0__')"
      ],
      "metadata": {
        "id": "Z23HvQ_eQM7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.rename_columns({\"prompt\": \"review\"})\n",
        "dataset = dataset.map(lambda x: {\"review\": x[\"review\"][:1000]}, batched=False)"
      ],
      "metadata": {
        "id": "cI0FCYN0Qh5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_in_len = 5\n",
        "txt_out_len = 32\n",
        "seed = 1\n",
        "\n",
        "dataset = dataset.map(\n",
        "    lambda x: {\"input_ids\": tokenizer.encode(\" \" + x[\"answer2\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=32)[0]},\n",
        "    batched=False,\n",
        ")\n",
        "dataset = dataset.map(lambda x: {\"query\": tokenizer.decode(x[\"input_ids\"])}, batched=False)\n",
        "dataset = dataset[:20480]\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_dict(dataset)\n",
        "dataset.set_format(\"pytorch\")"
      ],
      "metadata": {
        "id": "KyXqlD14SYbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "3vphOK1KT-6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.rename_column('answer2', 'chosen')\n",
        "dataset = dataset.rename_column('answer1', 'rejected')"
      ],
      "metadata": {
        "id": "fM2DXMbMUzbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "p4u1qACaU4lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
        "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"/content/reward_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "srjBNsoJTHGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
        "\n",
        "# trl: Transformer Reinforcement Learning library\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
        "from trl import create_reference_model\n",
        "from trl.core import LengthSampler\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# tqdm library makes the loops show a smart progress meter.\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "YVBlmjYixyVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=1.41e-5\n",
        "max_ppo_epochs=5\n",
        "mini_batch_size=4\n",
        "batch_size=1\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=\"tiiuae/falcon-7b-instruct\",\n",
        "    learning_rate=learning_rate,\n",
        "    ppo_epochs=max_ppo_epochs,\n",
        "    mini_batch_size=mini_batch_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "ppo_trainer = PPOTrainer(config=config,\n",
        "                         model=model,\n",
        "                         ref_model=ref_model,\n",
        "                         tokenizer=tokenizer,\n",
        "                         dataset=dataset,\n",
        "                         data_collator=collator)"
      ],
      "metadata": {
        "id": "X98cF9capZTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_score(model, tokenizer, responses):\n",
        "    positive_logist = []\n",
        "    for i in responses:\n",
        "        instructions = tokenizer.encode_plus(\n",
        "                                           i,\n",
        "                                           truncation=True,\n",
        "                                          padding=\"max_length\",\n",
        "                                          max_length=512,\n",
        "                                          return_tensors=\"pt\",\n",
        "                                      ).to(\"cuda:0\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**instructions)\n",
        "\n",
        "        logits = outputs[0].mean()\n",
        "        positive_logist.append(logits)\n",
        "\n",
        "    return positive_logist"
      ],
      "metadata": {
        "id": "BEu72LY0kujp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_min_length = 10\n",
        "output_max_length = 50\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"min_length\": 5,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True\n",
        "}\n",
        "\n",
        "reward_kwargs = {\n",
        "    \"top_k\": None,\n",
        "    \"function_to_apply\": \"none\", # You want the raw logits without softmax.\n",
        "    \"batch_size\": 5\n",
        "}\n",
        "\n",
        "max_ppo_steps = 10\n",
        "\n",
        "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    # Break when you reach max_steps.\n",
        "    if step >= max_ppo_steps:\n",
        "        break\n",
        "\n",
        "    prompt_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    summary_tensors = []\n",
        "\n",
        "    for prompt_tensor in prompt_tensors:\n",
        "        max_new_tokens = output_length_sampler()\n",
        "\n",
        "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
        "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
        "\n",
        "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
        "\n",
        "    # This needs to be called \"response\".\n",
        "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
        "\n",
        "    # Compute reward outputs.\n",
        "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "    logits = get_score(model, tokenizer,texts)\n",
        "\n",
        "    reward_tensors = [torch.tensor(logits)]\n",
        "\n",
        "    # Run PPO step.\n",
        "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
        "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
        "\n",
        "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
        "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
        "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
        "    print('-'.join('' for x in range(100)))"
      ],
      "metadata": {
        "id": "VWYHx4-DpZRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"rhlfmodel/\")\n",
        "tokenizer.save_pretrained(\"rhlfmodel/\")"
      ],
      "metadata": {
        "id": "SsB54PU9mJK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "model_path = \"rhlfmodel/\"\n",
        "set_seed(42)\n",
        "pipe = pipeline(\"text-generation\",model=model_path, tokenizer=model_path, max_length=30, num_return_sequences=1)"
      ],
      "metadata": {
        "id": "HtEg8ACHpZOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = dataset[11]\n",
        "text"
      ],
      "metadata": {
        "id": "WbmCtg_TpZLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(text['review'])"
      ],
      "metadata": {
        "id": "4x9Yk1-npZJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2rvoKJVEpZGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bf3NzCxSpZD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cMxa3_7hpZBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FkpssX6spY-Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}